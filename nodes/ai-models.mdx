---
title: AI Chat Models
description: OpenAI, Anthropic Claude, Google Gemini, OpenRouter, Groq, and Cerebras integration
---

# AI Chat Models

MachinaOs supports six AI providers for chat completions, with models fetched dynamically from each provider's API.

## Available Providers

| Provider | Models | Best For |
|----------|--------|----------|
| OpenAI | GPT-4o, GPT-4 Turbo, o1, o3, o4-mini | General purpose, reasoning |
| Anthropic | Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku | Coding, analysis, extended thinking |
| Google | Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 2.0 Flash Thinking | Multimodal, long context |
| OpenRouter | 200+ models | Access multiple providers via single API |
| Groq | Llama, Mixtral, Qwen | Ultra-fast inference |
| Cerebras | Llama, Qwen | Ultra-fast on custom hardware |

## Adding API Keys

1. Click the **key icon** in the toolbar
2. Select the provider
3. Enter your API key
4. Click **Validate** to test

<Info>
API keys are encrypted and stored locally. They're never sent to MachinaOs servers.
</Info>

---

## OpenAI Chat Model

### Models

| Model | Best For |
|-------|----------|
| gpt-4o | Most capable, multimodal |
| gpt-4-turbo | Fast, cost-effective GPT-4 |
| o1 | Complex reasoning tasks |
| o3 | Advanced reasoning |
| o4-mini | Fast, efficient reasoning |

### Parameters

<ParamField path="model" type="select" required>
  The model to use
</ParamField>

<ParamField path="prompt" type="string" required>
  The message to send. Supports template variables.
</ParamField>

<ParamField path="temperature" type="slider" default="0.7">
  Randomness (0 = deterministic, 1 = creative)
</ParamField>

<ParamField path="maxTokens" type="number" default="1000">
  Maximum response length
</ParamField>

<ParamField path="responseFormat" type="select" default="text">
  Output format: text or json_object
</ParamField>

<ParamField path="reasoningEffort" type="select" default="medium">
  For o-series models: minimal, low, medium, or high reasoning effort
</ParamField>

### Output

```json
{
  "response": "The AI's response text",
  "model": "gpt-4o",
  "thinking": "Reasoning process (o-series only)",
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 100,
    "total_tokens": 150
  }
}
```

---

## Anthropic Claude Model

### Models

| Model | Best For |
|-------|----------|
| claude-3-5-sonnet-20241022 | Best for coding and complex tasks |
| claude-3-opus-20240229 | Most capable, detailed analysis |
| claude-3-haiku-20240307 | Fast responses, simple tasks |

### Parameters

<ParamField path="model" type="select" required>
  Claude model to use
</ParamField>

<ParamField path="prompt" type="string" required>
  The message to send
</ParamField>

<ParamField path="systemPrompt" type="string">
  System instructions for the model
</ParamField>

<ParamField path="temperature" type="slider" default="0.7">
  Randomness (0-1)
</ParamField>

<ParamField path="maxTokens" type="number" default="1000">
  Maximum response length
</ParamField>

<ParamField path="thinkingEnabled" type="boolean" default="false">
  Enable extended thinking mode (Claude 3.5 Sonnet, Claude 3 Opus)
</ParamField>

<ParamField path="thinkingBudget" type="number" default="2048">
  Token budget for thinking (1024-16000). Shown when thinkingEnabled is true.
</ParamField>

### Extended Thinking

Claude's extended thinking mode shows the model's reasoning process:

```json
{
  "response": "Claude's final response",
  "thinking": "Let me analyze this step by step...",
  "model": "claude-3-5-sonnet-20241022",
  "stop_reason": "end_turn"
}
```

<Warning>
When thinking is enabled, `max_tokens` must be greater than `thinkingBudget`. Temperature is automatically set to 1.
</Warning>

---

## Google Gemini Model

### Models

| Model | Best For |
|-------|----------|
| gemini-2.5-pro | Most intelligent, complex tasks |
| gemini-2.5-flash | Fast, frontier performance |
| gemini-2.0-flash-thinking | Reasoning with thinking output |

### Parameters

<ParamField path="model" type="select" required>
  Gemini model to use
</ParamField>

<ParamField path="prompt" type="string" required>
  The message to send
</ParamField>

<ParamField path="temperature" type="slider" default="0.7">
  Randomness (0-1)
</ParamField>

<ParamField path="maxTokens" type="number" default="1000">
  Maximum response length
</ParamField>

<ParamField path="safetySettings" type="select" default="default">
  Content safety level
</ParamField>

<ParamField path="thinkingEnabled" type="boolean" default="false">
  Enable thinking mode (Gemini 2.5 models, Flash Thinking)
</ParamField>

### Output

```json
{
  "response": "Gemini's response",
  "thinking": "Reasoning process (when enabled)",
  "model": "gemini-2.5-pro"
}
```

---

## OpenRouter Model

OpenRouter provides access to 200+ models from multiple providers through a single API.

### Features

- **Unified API**: One API key for OpenAI, Anthropic, Google, Meta, Mistral, and more
- **Free Models**: Some models available at no cost (marked with [FREE] prefix)
- **Fallback**: Automatic model fallback if primary is unavailable

### Models

Models are grouped by cost in the dropdown:
- **Free models**: [FREE] prefix, no cost
- **Paid models**: Standard pricing per provider

Popular models include:
- `openai/gpt-4o`
- `anthropic/claude-3.5-sonnet`
- `google/gemini-2.5-pro`
- `meta-llama/llama-3.1-405b-instruct`
- `mistralai/mixtral-8x22b-instruct`

### Parameters

<ParamField path="model" type="select" required>
  Model in format: provider/model-name
</ParamField>

<ParamField path="prompt" type="string" required>
  The message to send
</ParamField>

<ParamField path="temperature" type="slider" default="0.7">
  Randomness (0-1)
</ParamField>

<ParamField path="maxTokens" type="number" default="1000">
  Maximum response length
</ParamField>

### Output

```json
{
  "response": "Model's response",
  "model": "openai/gpt-4o",
  "provider": "openrouter"
}
```

---

## Groq Model

Groq provides ultra-fast inference on custom LPU (Language Processing Unit) hardware.

### Models

| Model | Best For |
|-------|----------|
| llama-3.1-70b-versatile | General purpose, fast |
| llama-3.1-8b-instant | Ultra-fast, simple tasks |
| mixtral-8x7b-32768 | Long context, reasoning |
| qwen3-32b | Reasoning with parsed output |
| qwq-32b | Advanced reasoning |

### Parameters

<ParamField path="model" type="select" required>
  Groq model to use
</ParamField>

<ParamField path="prompt" type="string" required>
  The message to send
</ParamField>

<ParamField path="temperature" type="slider" default="0.7">
  Randomness (0-1)
</ParamField>

<ParamField path="maxTokens" type="number" default="1000">
  Maximum response length
</ParamField>

<ParamField path="reasoningFormat" type="select" default="parsed">
  For Qwen3/QwQ models: "parsed" returns reasoning, "hidden" returns only final answer
</ParamField>

### Reasoning Output

Qwen3 and QwQ models support reasoning output:

```json
{
  "response": "The final answer",
  "thinking": "Step-by-step reasoning process",
  "model": "qwen3-32b"
}
```

---

## Cerebras Model

Cerebras provides ultra-fast inference on custom wafer-scale AI hardware.

### Models

| Model | Best For |
|-------|----------|
| llama3.1-8b | Fast, efficient |
| llama3.1-70b | Capable, balanced |
| qwen-2.5-32b | Reasoning tasks |

### Parameters

<ParamField path="model" type="select" required>
  Cerebras model to use
</ParamField>

<ParamField path="prompt" type="string" required>
  The message to send
</ParamField>

<ParamField path="temperature" type="slider" default="0.7">
  Randomness (0-1)
</ParamField>

<ParamField path="maxTokens" type="number" default="1000">
  Maximum response length
</ParamField>

### Output

```json
{
  "response": "Cerebras model response",
  "model": "llama3.1-70b"
}
```

---

## Thinking/Reasoning Modes

Several providers support extended thinking or reasoning modes that show the model's internal reasoning process.

| Provider | Models | Parameter |
|----------|--------|-----------|
| Claude | 3.5 Sonnet, 3 Opus | thinkingBudget (tokens) |
| Gemini | 2.5 Pro/Flash, Flash Thinking | thinkingBudget (tokens) |
| OpenAI | o1, o3, o4 series | reasoningEffort (level) |
| Groq | Qwen3, QwQ | reasoningFormat (parsed/hidden) |

### Using Thinking Output

The `thinking` field is available in the node output for downstream nodes:

```
{{openaiChatModel.thinking}}
{{anthropicChatModel.thinking}}
```

---

## Comparing Providers

| Feature | OpenAI | Claude | Gemini | OpenRouter | Groq | Cerebras |
|---------|--------|--------|--------|------------|------|----------|
| Speed | Fast | Medium | Fast | Varies | Ultra-fast | Ultra-fast |
| Reasoning | o-series | Extended thinking | Thinking mode | Model-dependent | Qwen3/QwQ | - |
| Context Window | 128K | 200K | 1M+ | Varies | 32K-128K | 128K |
| Multimodal | Yes | Yes | Yes | Model-dependent | No | No |
| JSON Mode | Yes | No | No | Model-dependent | No | No |

---

## Common Use Cases

### Text Generation
```
Prompt: Write a product description for: {{input.product_name}}
Temperature: 0.8
```

### Data Extraction
```
Prompt: Extract the email and phone from: {{input.text}}
Response Format: json_object
Temperature: 0
```

### Complex Reasoning (with thinking)
```
Model: claude-3-5-sonnet
Thinking Enabled: true
Thinking Budget: 4096
Prompt: Analyze this code and explain the bug: {{input.code}}
```

---

## Tips

<Tip>
Use **temperature 0** for deterministic outputs like data extraction.
</Tip>

<Tip>
Use **temperature 0.7-0.9** for creative writing tasks.
</Tip>

<Tip>
Enable **thinking mode** for complex reasoning tasks that benefit from step-by-step analysis.
</Tip>

<Tip>
Use **OpenRouter** to experiment with different models without managing multiple API keys.
</Tip>

<Warning>
API calls cost money. Monitor your usage in your provider's dashboard.
</Warning>

---

## Error Handling

| Error | Cause | Solution |
|-------|-------|----------|
| 401 Unauthorized | Invalid API key | Check/update API key |
| 429 Rate Limited | Too many requests | Add delay, reduce frequency |
| 500 Server Error | Provider issue | Retry later |

---

## Related

<CardGroup cols={2}>
  <Card title="AI Agent" icon="robot" href="/nodes/ai-agent">
    Use models with memory and tools
  </Card>
  <Card title="AI Skills" icon="wand-magic-sparkles" href="/nodes/skills">
    Extend Chat Agent capabilities
  </Card>
  <Card title="AI Tools" icon="screwdriver-wrench" href="/nodes/tools">
    Tool nodes for AI agents
  </Card>
  <Card title="AI Tutorial" icon="graduation-cap" href="/tutorials/ai-agent-workflow">
    Build an AI-powered workflow
  </Card>
</CardGroup>
